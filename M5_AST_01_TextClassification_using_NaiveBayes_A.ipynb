{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saman1295/BikeRental/blob/main/M5_AST_01_TextClassification_using_NaiveBayes_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSstSEIv155v"
      },
      "source": [
        "# Advanced Certification Programme in AI and MLOps\n",
        "## A programme by IISc and TalentSprint\n",
        "### Assignment 1: Text Classification using Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_b5pYsPY7f3"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7sU_4_aZASD"
      },
      "source": [
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* perform different text processing techniques such as removing html strips and noise text, removing special characters, lemmatization, stemming, tokenization, removing stop words\n",
        "* train and evaluate a Naive Bayes model from the sklearn ML library, to predict the sentiment ('positive' or 'negative') for a movie reviews dataset\n",
        "* use the gradio library  to generate a customizable UI for displaying the predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ9aiXQDBQ-r"
      },
      "source": [
        "## Dataset description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEarccunBsDs"
      },
      "source": [
        "The [IMDB Movie Reviews dataset](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews. Only highly polarizing reviews are considered. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. No more than 30 reviews are included per movie. The dataset contains additional unlabeled data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZLYIcdJffR4"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2300920\" #@param {type:\"string\"}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY9tgfHgffSY"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"8130296147\" #@param {type:\"string\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Hlj81lsuffSZ",
        "outputId": "10601c28-911c-443a-e17e-7946e5ffc56e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M5_AST_01_TextClassification_using_NaiveBayes_A\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/CDS/Datasets/IMDB_Dataset.csv\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://aimlops-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2300920&recordId=2148\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5VyFvjMT5O-"
      },
      "source": [
        "### Importing Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZWtHdmPYMOvT",
        "outputId": "249930ae-c5f0-4a3c-e638-39452818ee6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer                        # to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer                        # to transform text into a term and document frequency based representation of numbers\n",
        "import nltk                                                                        # platform for building Python programs to process natural language\n",
        "nltk.download('stopwords')                                                         # to download the stop words\n",
        "nltk.download('punkt')                                                             # tokenizer divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences\n",
        "nltk.download('wordnet')                                                           # to lemmatize word using WordNet's built-in function\n",
        "from nltk.corpus import stopwords                                                  # importing the NTLK stopwords to remove articles, preposition and other words that are not actionable\n",
        "from nltk.stem.porter import PorterStemmer                                         # process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words\n",
        "from wordcloud import WordCloud                                                    # visualization of words based on their frequency\n",
        "from nltk.tokenize import word_tokenize                                            # allows to create individual objects from a bag of words\n",
        "from bs4 import BeautifulSoup                                                      # Python library for pulling data from HTML and XML files\n",
        "import re                                                                          # regular expression (or RE) specifies a set of strings that matches it\n",
        "from sklearn.naive_bayes import MultinomialNB                                      # to import multinomial naive bayes which is suitable for classification with discrete features (e.g., word counts for text classification)\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score  # to import metrics for evaluating the classification model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7bG2m775ba6"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_fAB1U0fqJ7"
      },
      "outputs": [],
      "source": [
        "# read the dataset\n",
        "df = pd.read_csv('IMDB_Dataset.csv')\n",
        "print(df.shape)\n",
        "df.head(10)      # first 10 rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy2WsjbRDCJr"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8mLwZe5CWSN"
      },
      "outputs": [],
      "source": [
        "# summary of the dataset\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twI5ntekDNrs"
      },
      "source": [
        "Now, we will look at the sentiment count by category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6izwMrUVDQ6f"
      },
      "outputs": [],
      "source": [
        "# sentiment count\n",
        "# YOUR CODE HERE to show the count of each category in sentiment comlun"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyeDUn36DVRn"
      },
      "source": [
        "We can see that the dataset is balanced.\n",
        "\n",
        "Now, we will do the text cleaning of the reviews.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on2XCsxtI0-b"
      },
      "source": [
        "### Text Cleaning\n",
        "\n",
        "The data scraped from the website is mostly in the raw text form. This data needs to be cleaned before analyzing it or fitting a model to it. Cleaning up the text data is necessary to highlight the attributes that we are going to want our machine learning system to pick up on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tupdRoUSGYrw"
      },
      "source": [
        "**Removing noisy text**\n",
        "\n",
        "Sample noise removal tasks could include:\n",
        "\n",
        "* removing text file headers, footers\n",
        "* removing HTML, XML, etc. markup and metadata\n",
        "* extracting valuable data from other formats, such as JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzsW7umy6Sea"
      },
      "outputs": [],
      "source": [
        "# removing the html strips\n",
        "def strip_html(text):\n",
        "    # BeautifulSoup is a useful library for extracting data from HTML and XML documents\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "# removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "# removing the noisy text\n",
        "def denoise_text(text):\n",
        "    # YOUR CODE HERE to update the text by applying strip_html() function\n",
        "    # YOUR CODE HERE to update the text by applying remove_between_square_brackets() function\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x56E-c6KFvZH"
      },
      "outputs": [],
      "source": [
        "# apply denoise_text() function on review column\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQzlTJ-VGwVu"
      },
      "source": [
        "**Removing special characters**\n",
        "\n",
        "Special characters typically include any character that is not a letter or number, such as punctuation. Removing special characters from a string results in a string containing only letters and numbers.\n",
        "\n",
        "We can use the `re` python library for Regular expression operations.\n",
        "\n",
        "To know more about Regular expressions, click [here](https://realpython.com/regex-python/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsi1MZ9X6Xqm"
      },
      "outputs": [],
      "source": [
        "# define function for removing special characters\n",
        "def remove_special_characters(text):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]'\n",
        "    text = re.sub(pattern,'',text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBWV2c7oGpVL"
      },
      "outputs": [],
      "source": [
        "# apply remove_special_characters() function on review column\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKHDo92_nm-H"
      },
      "source": [
        "**Lemmatization**\n",
        "\n",
        "Lemmatization is a text pre-processing technique used to break a word down to its root meaning or word (called lemme) to identify similarities.\n",
        "\n",
        "For example, a lemmatization algorithm would reduce the word ***better*** to its root word, or lemme, ***good***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjqMtkJ9ake5"
      },
      "outputs": [],
      "source": [
        "# Lemmatize word using WordNet's built-in function\n",
        "# pos: The Part Of Speech tag.\n",
        "#      Valid options are \"n\" for nouns, \"v\" for verbs, \"a\" for adjectives,\n",
        "#                        \"r\" for adverbs and \"s\" for satellite adjectives.\n",
        "\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\", ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGsYJGmLG9bM"
      },
      "source": [
        "**Text Stemming**\n",
        "\n",
        "Stemming, also called suffix stripping, is a technique used to reduce text dimensionality. Stemming is also a type of text normalization that enables you to standardize some words into specific expressions also called stems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R8eAA_b6bOS"
      },
      "outputs": [],
      "source": [
        "# stemming the text\n",
        "def simple_stemmer(text):\n",
        "    ps = nltk.porter.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmBWpKUhG5F7"
      },
      "outputs": [],
      "source": [
        "# apply function on review column\n",
        "# YOUR CODE HERE to apply 'simple_stemmer' function on reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA2RsIUenbGe"
      },
      "source": [
        "**Tokenization**\n",
        "\n",
        "Tokenization is the process of splitting paragraphs and sentences into smaller understandable parts (words).\n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbCqVxg_F6jw"
      },
      "outputs": [],
      "source": [
        "word_tokenize('This is a test sentence.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omeXdqsFHm1M"
      },
      "source": [
        "**Removing stopwords**\n",
        "\n",
        "Stopwords are English words that do not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgIjbfZgEgNH"
      },
      "outputs": [],
      "source": [
        "# setting english stopwords\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "print(stopword_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jacr7HAjfILU"
      },
      "source": [
        "The above list of stopwords also contains the word \"not\", and its other forms such as don't, didn't, etc. We need them for correct sentiment classification.\n",
        "\n",
        "For example, consider a negative review \"*not a good movie*\", and if we remove 'not' from it then it becomes a positive review \"*a good movie*\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1uSxBepgdsE"
      },
      "outputs": [],
      "source": [
        "# Exclude 'not' and its other forms from the stopwords list\n",
        "\n",
        "updated_stopword_list = []\n",
        "\n",
        "for word in stopword_list:\n",
        "    if word=='not' or word.endswith(\"n't\"):\n",
        "        pass\n",
        "    else:\n",
        "        updated_stopword_list.append(word)\n",
        "\n",
        "print(updated_stopword_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fH5HXfk8OJj"
      },
      "outputs": [],
      "source": [
        "# removing the stopwords\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    # splitting strings into tokens (list of words)\n",
        "\n",
        "    # YOUR CODE HERE to create 'tokens' variable by tokenizing 'text'\n",
        "\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        # filtering out the stop words\n",
        "        filtered_tokens = [token for token in tokens if token not in updated_stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in updated_stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_OBq2crHXU5"
      },
      "outputs": [],
      "source": [
        "# apply remove_stopwords function on review column\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxWPUIt3QrBY"
      },
      "source": [
        "After cleaning the reviews, we now split the clean dataset into training and testing set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO5JxX7B8ita"
      },
      "source": [
        "### Split into training and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juVRVeO6i3Xp"
      },
      "source": [
        "**Parameters in train_test_split**\n",
        "\n",
        "* arrays sequence of indexables with same length / shape (here, df.review and df.sentiment):\n",
        "Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.\n",
        "\n",
        "* test_size (float or int, default=None): The proportion of the dataset to include in the test split If train_size is also None, it will be set to 0.25.\n",
        "\n",
        "\n",
        "* RandomState:\n",
        "Controls the shuffling applied to the data before applying the split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwGbAazvDVuB"
      },
      "outputs": [],
      "source": [
        "# Split into training and testing set\n",
        "X_train, X_test, y_train, y_test = # YOUR CODE HERE to split data with test_size = 0.2 and random_state = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_LwQzG3Ec3i"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG_BKf2kT9Ui"
      },
      "source": [
        "Let us see positive and negative words by using **WordCloud**.\n",
        "\n",
        "A word cloud (also called tag cloud or weighted list) is a visual representation of text data. Words are usually single words, and the importance of each is shown with font size or color.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upS9PJwNTYF5"
      },
      "outputs": [],
      "source": [
        "# Word cloud for positive review words\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "df_positive_review =  df[df['sentiment']=='positive']\n",
        "positive_text = ' '.join(review for review in df_positive_review.review)\n",
        "WC = WordCloud(width=1000, height=500, max_words=500, min_font_size=5)\n",
        "positive_words = WC.generate(positive_text)\n",
        "plt.imshow(positive_words, interpolation='bilinear')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRkYhLgjULMH"
      },
      "outputs": [],
      "source": [
        "# Word cloud for negative review words\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOWWCH-gLJ0k"
      },
      "source": [
        "## Converting Text into numerical feature vectors:\n",
        "\n",
        "### 1. Bag of Words (based on word occurrence)\n",
        "\n",
        "In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors.\n",
        "\n",
        "Bags of words is the most intuitive way to create such a representation:\n",
        "\n",
        "Assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices).\n",
        "\n",
        "For each document $i$, count the number of occurrences of each word $w$ and store it in $X[i, j]$ as the value of feature $j$ where $j$ is the index of word $w$ in the dictionary.\n",
        "\n",
        "The bags of words representation implies that n_features is the number of distinct words in the corpus.\n",
        "\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/bag-of-words-example.png\" width=500px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO52_AYgaMZa"
      },
      "source": [
        "#### Implementing Bag of Words using sklearn `Count Vectorizer`\n",
        "\n",
        "The `CountVectorizer` provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n",
        "\n",
        "An encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document. The vectors returned from a call to transform() will be sparse vectors, and we can transform them back to numpy arrays to look and better understand what is going on by calling the `toarray()` function.\n",
        "\n",
        "To know more about CountVectorizer, click [here](https://towardsdatascience.com/basics-of-countvectorizer-e26677900f9c)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilmOsl0ciO88"
      },
      "outputs": [],
      "source": [
        "# Count vectorizer\n",
        "cv = CountVectorizer(min_df=0, max_df=1, binary=False, ngram_range=(1,3))\n",
        "\n",
        "# transformed train reviews\n",
        "cv_train_reviews = cv.fit_transform(X_train)\n",
        "\n",
        "# transformed test reviews\n",
        "cv_test_reviews = # YOUR CODE HERE to transform X_test\n",
        "\n",
        "print('CV_train:', cv_train_reviews.shape)\n",
        "print('CV_test:', cv_test_reviews.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN95C2gCM_Hc"
      },
      "source": [
        "### 2. TF-IDF (based on word frequencies)\n",
        "\n",
        "The issue with occurrence count is that longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
        "\n",
        "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\n",
        "\n",
        "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
        "\n",
        "This downscaling is called tf–idf for “Term Frequency times Inverse Document Frequency”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAavZUbLPNoY"
      },
      "source": [
        "#### Implementing TF-IDF with sklearn `TfidfVectorizer`\n",
        "\n",
        "It is used to convert text documents to matrix of tf-idf features.\n",
        "\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/tf-idf-formula.PNG\" width=500px>\n",
        "\n",
        "`TfidfVectorizer` is the base building block of many NLP pipelines. It is a simple technique to vectorize text documents — i.e. transform sentences into arrays of numbers — and use them in subsequent tasks.\n",
        "\n",
        "To know more about tf-idf, click [here]( https://cdn.iisc.talentsprint.com/CDS/TF-IDF.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm4CdvwfLbLi"
      },
      "outputs": [],
      "source": [
        "# tfidf vectorizer\n",
        "tv = TfidfVectorizer(min_df=0, max_df=1, use_idf=True, ngram_range = (1,3))\n",
        "\n",
        "# transformed train reviews\n",
        "tfidf_train_reviews = # YOUR CODE HERE to fit and transform X_train\n",
        "\n",
        "# transformed test reviews\n",
        "tfidf_test_reviews = # YOUR CODE HERE to transform X_test\n",
        "\n",
        "print('Tfidf_train:', tfidf_train_reviews.shape)\n",
        "print('Tfidf_test:', tfidf_test_reviews.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne_S1VjfRH_S"
      },
      "source": [
        "### Modeling the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA2VTJIOMb1n"
      },
      "source": [
        "**Naïve Bayes’ Classifier**\n",
        "\n",
        "Bayesian network classifiers are a popular supervised\n",
        "classification paradigm. The Naïve Bayes’ classifier is a probabilistic\n",
        "classifier based on the Bayes’ theorem, considering a 'naive'\n",
        "independence assumption.\n",
        "It is a popular(baseline) method for text categorization, with word frequencies as the feature.\n",
        "\n",
        "An advantage of Naïve Bayes’ is that it\n",
        "only requires a small amount of training data to\n",
        "estimate the parameters necessary for classification. Despite its simplicity and strong assumptions, the\n",
        "Naïve Bayes’ classifier has been proven to work\n",
        "satisfactorily in many domains. Bayesian classification\n",
        "provides practical learning algorithms and prior knowledge and observed data can be combined. In Naïve Bayes’ technique, the basic idea is to find the\n",
        "probabilities of categories given a text document by\n",
        "using the joint probabilities of words and categories. It is based on the assumption of word independence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9oco2hkRXP3"
      },
      "source": [
        "Naive Bayes classifier for multinomial models.\n",
        "\n",
        "The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\n",
        "\n",
        "To know more, click [here](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkqhHOOpjxni"
      },
      "outputs": [],
      "source": [
        "# training the model\n",
        "mnb = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
        "\n",
        "# fitting the NaiveBayes for count vectorizer\n",
        "mnb_cv = mnb.fit(cv_train_reviews, y_train)\n",
        "\n",
        "print('MultinomialNB for Count Vectorizer :', mnb_cv)\n",
        "\n",
        "# fitting the NaiveBayes for tfidf features\n",
        "mnb_tfidf = mnb.fit(tfidf_train_reviews, y_train)\n",
        "\n",
        "print('MultinomialNB for tf-idf :', mnb_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU0rl3M8n3_d"
      },
      "source": [
        "**Optional deeper dive:**\n",
        "\n",
        "Naive Bayes model parameters:\n",
        "\n",
        "* *alpha*:\n",
        "Additive smoothing parameter (0 for no smoothing). In statistics, additive smoothing, also called Laplace smoothing, is a technique used to smooth categorical data. Given a set of observation counts ${\\textstyle {\\mathbf {x} \\ =\\ \\left\\langle x_{1},\\,x_{2},\\,\\ldots ,\\,x_{d}\\right\\rangle }}$ from a $d$-dimensional multinomial distribution with $N$ trials, a \"smoothed\" version of the counts gives the estimator:\n",
        "$${\\hat {\\theta }}_{i}={\\frac {x_{i}+\\alpha }{N+\\alpha d}}\\qquad (i=1,\\ldots ,d),$$\n",
        "where the smoothed count ${\\textstyle {{\\hat {x}}_{i}=N{\\hat {\\theta }}_{i}}}$ and the \"pseudocount\" $α > 0$ is a smoothing parameter. $α = 0$ corresponds to no smoothing.\n",
        "\n",
        "* *fit_prior*:\n",
        "Whether to learn class prior probabilities or not. If false, a uniform prior will be used.\n",
        "\n",
        "* *class_prior*:\n",
        "Prior probabilities of the classes. If specified the priors are not adjusted according to the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDp3YW0fSAtO"
      },
      "source": [
        "**Model performance on test data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnXIgKkLR2uM"
      },
      "outputs": [],
      "source": [
        "# predicting the model for CountVectorizer\n",
        "mnb_cv_predict = mnb.predict(cv_test_reviews)\n",
        "print('predictions for Count Vectorizer :', mnb_cv_predict)\n",
        "\n",
        "# predicting the model for tfidf features\n",
        "mnb_tfidf_predict = mnb.predict(tfidf_test_reviews)\n",
        "print('predictions for tf-idf :', mnb_tfidf_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUnQOGpoShcz"
      },
      "source": [
        "**Accuracy of Model**\n",
        "\n",
        "It is the ratio of number of correct classifications to the total number of input samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1EHCAnWSRZ1"
      },
      "outputs": [],
      "source": [
        "# accuracy score for count vectorizer\n",
        "mnb_cv_score = accuracy_score(y_test, mnb_cv_predict)\n",
        "print(\"mnb_cv_score :\", mnb_cv_score)\n",
        "\n",
        "# accuracy score for tf-idf\n",
        "mnb_tfidf_score = # YOUR CODE HERE\n",
        "print(\"mnb_tfidf_score :\", mnb_tfidf_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEih5xUNTFX3"
      },
      "source": [
        "**Plotting the confusion matrix**\n",
        "\n",
        "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known.\n",
        "\n",
        "A confusion matrix, in predictive analytics, is a square matrix that tells us the rate of false positives, false negatives, true positives and true negatives for a test or predictor. We can make a confusion matrix if we know both the predicted values and the true values for a sample set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWeRBlGBmIHB"
      },
      "outputs": [],
      "source": [
        "# confusion matrix for count vectorizer\n",
        "cm_cv = confusion_matrix(y_test, mnb_cv_predict, labels=['positive', 'negative'])\n",
        "print('confusion matrix for count vectorizer :\\n', cm_cv)\n",
        "\n",
        "# confusion matrix for tf-idf\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXQYvbik134w"
      },
      "source": [
        "## Gradio Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9LGFxtQjwxv"
      },
      "source": [
        "Gradio is an open-source python library that allows us to quickly create easy-to-use, customizable UI components for our ML model, any API, or any arbitrary function in just a few lines of code. We can integrate the GUI directly into the Python notebook, or we can share the link with anyone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl0ngdnKjKHH"
      },
      "outputs": [],
      "source": [
        "!pip -qq install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XwCJSdDDVVU"
      },
      "outputs": [],
      "source": [
        "import gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U41iWl1H14kt"
      },
      "outputs": [],
      "source": [
        "# Function for preprocessing of text\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    text = denoise_text(text)\n",
        "    text = remove_special_characters(text)\n",
        "    text = simple_stemmer(text)\n",
        "    text = remove_stopwords(text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTEAHUtr0vm1"
      },
      "outputs": [],
      "source": [
        "# Function to predict label for a review\n",
        "\n",
        "def predict_review_label(text, vectorizer_method):\n",
        "\n",
        "    processed_text = preprocess_text(text)\n",
        "\n",
        "    if vectorizer_method == 'CountVectorizer':\n",
        "        review = cv.transform([processed_text])\n",
        "        pred = mnb_cv.predict(review)\n",
        "    if vectorizer_method == 'TFIDFVectorizer':\n",
        "        review = # YOUR CODE HERE to transform 'processed_text' using TfidfVectorizer\n",
        "        pred = mnb_tfidf.predict(review)\n",
        "    else:\n",
        "        review = cv.transform([processed_text])\n",
        "        pred = mnb_cv.predict(review)\n",
        "\n",
        "    return pred[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG4q5-9I6n0x"
      },
      "outputs": [],
      "source": [
        "# Testing a review\n",
        "predict_review_label(\"It was a good movie, really enjoyed it a lot.\", 'CountVectorizer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b17nuGAm-hym"
      },
      "outputs": [],
      "source": [
        "# Testing a review\n",
        "predict_review_label(\"Was very bad, I was barely able to understand the concepts.\", 'TFIDFVectorizer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAWKOcY25sDA"
      },
      "outputs": [],
      "source": [
        "# Dropdown choices\n",
        "in_vectorizer_dropdown = gradio.Dropdown(['CountVectorizer', 'TFIDFVectorizer'], type=\"value\", label='Choose a Method to Vectorize')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy_0yXHI7fs4"
      },
      "outputs": [],
      "source": [
        "# Input from user\n",
        "in_review = gradio.Textbox(lines=2, placeholder=None, value=\"review\", label='Enter Review Text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlLlAp4E8pew"
      },
      "outputs": [],
      "source": [
        "# Output prediction\n",
        "out_label = gradio.Textbox(type=\"text\", label='Predicted Review Label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpRnUY5w551-"
      },
      "outputs": [],
      "source": [
        "# Gradio interface to generate UI link\n",
        "\n",
        "iface = gradio.Interface(fn = predict_review_label,\n",
        "                         inputs = [in_review, in_vectorizer_dropdown],\n",
        "                         outputs = [out_label])\n",
        "\n",
        "iface.launch(share = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2eQkcvnvntC"
      },
      "source": [
        "Click on the link generated above to see UI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VgSwVENIPcM6"
      },
      "outputs": [],
      "source": [
        "#@title Which of the following is a technique to convert a word into its base form? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"LabelEncoding\", \"Stemming\", \"TF-IDF\", \"Naive Bayes algorithm\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NMzKSbLIgFzQ"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}